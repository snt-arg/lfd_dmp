% This file was created with JabRef 2.10.
% Encoding: ISO8859_1

@Article{buchli11learning,
  title                    = {Learning Variable Impedance Control},
  author                   = {Jonas Buchli and Freek Stulp and Evangelos Theodorou and Stefan Schaal},
  journal                  = {International Journal of Robotics Research},
  year                     = {2011},
  number                   = {7},
  pages                    = {820-833},
  volume                   = {30},
  doi                      = {10.1177/0278364911402527},

  abstract                 = {One of the hallmarks of the performance, versatility, and robustness of biological motor control is the ability to adapt the impedance of the overall biomechanical system to different task requirements and stochastic disturbances. A transfer of this principle to robotics is desirable, for instance to enable robots to work robustly and safely in everyday human environments. It is, however, not trivial to derive variable impedance controllers for practical high degree-of-freedom (DOF) robotic tasks. In this contribution, we accomplish such variable impedance control with the reinforcement learning (RL) algorithm PI2 ({\bf P}olicy {\bf I}mprovement with {\bf P}ath {\bf I}ntegrals). PI2 is a model-free, sampling based learning method derived from first principles of stochastic optimal control. The PI2 algorithm requires no tuning of algorithmic parameters besides the exploration noise. The designer can thus fully focus on cost function design to specify the task. From the viewpoint of robotics, a particular useful property of PI2 is that it can scale to problems of many DOFs, so that reinforcement learning on real robotic systems becomes feasible. We sketch the PI2 algorithm and its theoretical properties, and how it is applied to gain scheduling for variable impedance control. We evaluate our approach by presenting results on several simulated and real robots. We consider tasks involving accurate tracking through via-points, and manipulation tasks requiring physical contact with the environment. In these tasks, the optimal strategy requires both tuning of a reference trajectory \emph{and} the impedance of the end-effector. The results show that we can use path integral based reinforcement learning not only for planning but also to derive variable gain feedback controllers in realistic scenarios. Thus, the power of variable impedance control is made available to a wide variety of robotic systems and practical applications.},
  url                      = {http://ijr.sagepub.com/content/early/2011/03/31/0278364911402527}
}

@Article{ijspeert13dynamical,
  title                    = {{Dynamical Movement Primitives}: Learning Attractor Models for Motor Behaviors},
  author                   = {Ijspeert, A. and Nakanishi, J. and Pastor, P and Hoffmann, H. and Schaal, S.},
  journal                  = {Neural Computation},
  year                     = {2013},
  number                   = {2},
  pages                    = {328--373},
  volume                   = {25},
  doi                      = {10.1162/neco_a_00393},
}

@InProceedings{ijspeert02movement,
  title                    = {Movement imitation with nonlinear dynamical systems in humanoid robots},
  author                   = {A. J. Ijspeert and J. Nakanishi and S. Schaal},
  booktitle                = {Proceedings of the {IEEE} International Conference on Robotics and Automation ({ICRA})},
  doi                      = {10.1109/ROBOT.2002.1014739},
  year                     = {2002},
}

@InProceedings{kalakrishnan11learning,
  title                    = {Learning force control policies for compliant manipulation},
  author                   = {Kalakrishnan, M. and Righetti, L. and Pastor, P. and Schaal, S.},
  booktitle                = {{IEEE/RSJ} International Conference on Intelligent Robots and Systems ({IROS} 2011)},
  year                     = {2011},

  url                      = {http://www-clmc.usc.edu/publications/K/kalakrishnan-IROS2011}
}

@Article{kulvicius12joining,
  title                    = {Joining Movement Sequences: Modified Dynamic Movement Primitives for Robotics Applications Exemplified on Handwriting},
  author                   = {Tomas Kulvicius and KeJun Ning and Minija Tamosiunaite and Florentin W{\"o}rg{\"o}tter},
  journal                  = {IEEE Transactions on Robotics},
  year                     = {2012},
  number                   = {1},
  pages                    = {145-157},
  volume                   = {28},
  doi                      = {10.1109/tro.2011.2163863},

  bibsource                = {DBLP, http://dblp.uni-trier.de},
  ee                       = {http://dx.doi.org/10.1109/TRO.2011.2163863},
}

@Article{matsubara11learning,
  title                    = {Learning parametric dynamic movement primitives from multiple demonstrations},
  author                   = {Matsubara, T and Hyon, S and Morimoto, J},
  journal                  = {Neural Networks},
  year                     = {2011},
  number                   = {5},
  pages                    = {493-500},
  volume                   = {24},
  doi                      = {10.1016/j.neunet.2011.02.004},
}

@InProceedings{silva12learning,
  title                    = {Learning Parameterized Skills},
  author                   = {da Silva, Bruno and Konidaris, George and Barto, Andrew G.},
  booktitle                = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  year                     = {2012},

  address                  = {New York, NY, USA},
  editor                   = {John Langford and Joelle Pineau},
  month                    = {July},
  pages                    = {1679--1686},
  publisher                = {Omnipress},
  series                   = {ICML '12},
  isbn                     = {978-1-4503-1285-1},
  location                 = {Edinburgh, Scotland, GB},
}

@InProceedings{stulp12adaptive,
  title                    = {Adaptive Exploration for Continual Reinforcement Learning},
  author                   = {Freek Stulp},
  booktitle                = {International Conference on Intelligent Robots and Systems ({IROS})},
  year                     = {2012},
  doi                      = {10.1109/IROS.2012.6385818},
  pages                    = {1631-1636},

  abstract                 = {Most experiments on policy search for robotics focus on isolated tasks, where the experiment is split into two distinct phases: 1)~the learning phase, where the robot learns the task through exploration; 2)~the exploitation phase, where exploration is turned off, and the robot demonstrates its performance on the task it has learned. In this paper, we present an algorithm that enables robots to continually and autonomously alternate between these phases. We do so by combining the `Policy Improvement with Path Integrals' direct reinforcement learning algorithm with the covariance matrix adaptation rule from the `Cross-Entropy Method' optimization algorithm. This integration is possible because both algorithms iteratively update parameters with probability-weighted averaging. A practical advantage of the novel algorithm, called PI2-CMA, is that it alleviates the user from having to manually tune the degree of exploration. We evaluate PI2-CMA's ability to continually and autonomously tune exploration on two tasks.},
}

@InProceedings{stulp14simultaneous,
  title                    = {Simultaneous On-line Discovery and Improvement of Robotic Skill Options},
  author                   = {Freek Stulp and Laura Herlant and Antoine Hoarau and Gennaro Raiola},
  booktitle                = {International Conference on Intelligent Robots and Systems ({IROS})},
  year                     = {2014},
  doi                      = {10.1109/IROS.2014.6942741}, 
}

@InProceedings{stulp13learning,
  title                    = {Learning Compact Parameterized Skills with a Single Regression},
  author                   = {Freek Stulp and Gennaro Raiola and Antoine Hoarau and Serena Ivaldi and Olivier Sigaud},
  booktitle                = {{IEEE-RAS} International Conference on Humanoid Robots},
  year                     = {2013},
  doi                      = {10.1109/HUMANOIDS.2013.7030008}, 
}

@Article{stulp15many,
  title                    = {Many regression algorithms, one unified model -- A review},
  author                   = {Freek Stulp and Olivier Sigaud},
  journal                  = {Neural Networks},
  year                     = {2015},

  abstract                 = { Regression is the process of learning relationships between inputs and continuous outputs from example data, which enables predictions for novel inputs. The history of regression is closely related to the history of artificial neural networks since the seminal work of Rosenblatt (1958). The aims of this paper are to provide an overview of many regression algorithms, and to demonstrate how the function representation whose parameters they regress fall into two classes: a weighted sum of basis functions, or a mixture of linear models. Furthermore, we show that the former is a special case of the latter. Our ambition is thus to provide a deep understanding of the relationship between these algorithms, that, despite being derived from very different principles, use a function representation that can be captured within one unified model. Finally, step-by-step derivations of the algorithms from first principles and visualizations of their innerworkings allow this article to be used as a tutorial for those new to regression.},
  doi                      = {10.1016/j.neunet.2015.05.005},
  url                      = {http://www.sciencedirect.com/science/article/pii/S0893608015001185}
}

@Article{stulp13robot,
  title                    = {Robot Skill Learning: From Reinforcement Learning to Evolution Strategies},
  author                   = {Freek Stulp and Olivier Sigaud},
  journal                  = {Paladyn. Journal of Behavioral Robotics},
  year                     = {2013},

  month                    = {September},
  number                   = {1},
  pages                    = {49--61},
  volume                   = {4},
  doi                      = {10.2478/pjbr-2013-0003},

  abstract                 = {Due to trends towards searching in parameter space and using reward-weighted averaging, reinforcement learning (RL) algorithms for policy improvement are now able to learn sophisticated robot skills. A side-effect of these trends has been that RL algorithms have become more and more similar to evolution strategies, which treat policy improvement as a black-box optimization problem, and thus do not leverage the problem structure as RL algorithms do. We demonstrate how two straightforward simplifications to the state-of-the-art RL algorithm PI2 suffice to convert it into the black-box optimization algorithm (\mu_W,\lambda)-ES. Furthermore, we show that (\mu_W,\lambda)-ES empirically outperforms PI2 on several tasks. It is striking that PI2 and (\mu_W,\lambda)-ES share a common core, and that the simpler, older algorithm outperforms the more sophisticated, newer one. We argue that this is due to a third trend in robot skill learning: the predominant use of dynamic movement primitives (DMPs). We show how DMPs dramatically simplify the learning problem, and discuss the implications of this for past and future work on robot skill learning. },
}

@InProceedings{stulp12path,
  title                    = {Path Integral Policy Improvement with Covariance Matrix Adaptation},
  author                   = {Freek Stulp and Olivier Sigaud},
  booktitle                = {Proceedings of the 29th International Conference on Machine Learning (ICML)},
  year                     = {2012},

  abstract                 = {There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2- as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. At the conceptual level, we compare PI2 to other members of the same family, being Cross-Entropy Methods and CMAES. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for ``Path Integral Policy Improvement with Covariance Matrix Adaptation''. PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically},
}

@Unpublished{stulp12policy_hal,
  title                    = {Policy Improvement Methods: Between Black-Box Optimization and Episodic Reinforcement Learning},
  author                   = {Freek Stulp and Olivier Sigaud},
  note                     = {hal-00738463},
  year                     = {2012},

  url                      = {http://hal.archives-ouvertes.fr/hal-00738463}
}

